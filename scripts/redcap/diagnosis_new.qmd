---
title: "Predictive Analysis of FAI scales"
subtitle: "Development of the FAI Scale"
author: Corrado Caudek 
date: today
format:
  html:
    embed-resources: true
    standalone: true
    toc: true
    code-fold: show
    code-copy: true
    code-line-numbers: true
editor_options: 
  chunk_output_type: console
bibliography: refs.bib
execute:
  warning: false
  echo: true
---

## Background

This study aims to develop a psychological scale (FAI) to differentiate between family caregivers of children with chronic illnesses based on their coping abilities. This scale will identify caregivers who may benefit from additional support or interventions. 

In this analysis, the severity of illnesses (`nome_malattia_raggruppato`) will be used as the criterion variable to evaluate the classification ability of the FAI scales.

Key questions:

1. Are the FAI scales adequate predictors of illness severity?
2. How well do statistical and machine learning models perform in predicting severity?
3. What conclusions can be drawn about the utility of the FAI scales?


## Load packages and options

```{r}
if (!requireNamespace("pacman")) install.packages("pacman")

pacman::p_load(
  here, tidyverse, TAM, mirt, lavaan, mokken, psych, semTools, MASS, brant,
  ids, pROC, nnet, caret, ROSE, recipes, themis, randomForest, reshape2,
  VGAM, pdp, themis, smotefamily, brms, cmdstanr
)

# Disable significance stars globally
options(show.signif.stars = FALSE)
```

## Import and Preprocess Data

### Load Data


```{r}
# Set working directory for RedCap data
setwd("~/_repositories/meyer_fai/scripts/redcap")

# Source helper functions
source(here::here("scripts", "redcap", "_targets.R"))

# Read cleaned RedCap data
temp <- tar_read(fai_clean_complete)
# Remove problematic cases based on prior analysis
temp1 <- temp[-c(12, 18, 76, 81, 152, 182, 193, 202), ]
# Remove outlier identified previously
fai_clean_complete <- temp1[-92, ]
rm(temp, temp1)

# Select and rename relevant variables
fai <- fai_clean_complete |>
  dplyr::rename(
    fai_child_char = fai_child_charact,
    fai_caregiving = fai_caregiving,
    fai_percz_cure = fai_cure,
    fai_intra_psyc = fai_intrapsych,
    fai_soc_fam_sp = fai_support,
    fai_hyper_prot = fai_hyper
  )
```

```{r}
fai <- fai |>
  mutate(
    nome_malattia_raggruppato = case_when(
      nome_malattia %in% c(
        "glioma", "LLA", "linfoma anaplastico", "Leucemia mieloide",
        "fibrosi cistica", "Pinealoblastoma",
        "tumore glioneurale leptomeningeale diffuso"
      ) ~ "Molto grave",

      nome_malattia %in% c(
        "Epilessia", "sindrome di charge", "Distrofia muscolare",
        "Neurofibromatosi", "diabete di tipo 1", "artrite romatoide",
        "RCU", "emofilia"
      ) ~ "Grave",

      nome_malattia %in% c(
        "asma", "Emicrania", "dermatite atopica", "ipospadia",
        "ipotonia muscolare", "Blefaro chetato congiuntivite"
      ) ~ "Moderata",

      TRUE ~ "Lieve"
    )
  )

fai$nome_malattia_raggruppato <- factor(fai$nome_malattia_raggruppato)
fai$nome_malattia_raggruppato <-
  relevel(fai$nome_malattia_raggruppato, ref = "Lieve")

fai$nome_malattia <- NULL
```

```{r}
fai$malattia <- factor(fai$nome_malattia_raggruppato)
fai$nome_malattia_raggruppato <- NULL
```

```{r}
# Select the specified variables
selected_data <- fai %>%
  dplyr::select(dass_de_sc, fai_child_char, fai_caregiving, fai_percz_cure, 
         fai_intra_psyc, fai_soc_fam_sp, fai_hyper_prot, malattia, 
         dass_ax_sc, dass_str_sc)

# Scale the numeric variables (all except the categorical 'malattia')
scaled_data <- selected_data %>%
  mutate(across(
    .cols = c(dass_de_sc, fai_child_char, fai_caregiving, fai_percz_cure, 
              fai_intra_psyc, fai_soc_fam_sp, fai_hyper_prot, dass_ax_sc, dass_str_sc),
    .fns = ~ as.vector(scale(.)),
    .names = "{.col}"
  ))
```

```{r}
scaled_data$quanti_fratelli <- fai$quanti_fratelli
```

```{r}
d <- scaled_data |> 
  dplyr::select(starts_with("fai_"), malattia, quanti_fratelli, dass_tot)
outliers <- check_outliers(d, method = "mcd", verbose = FALSE)
outliers

d_clean <- d[-which(outliers), ]
```


```{r}
hist(scaled_data$dass_de_sc)
```

```{r}
mod_dep <- brm(
  dass_de_sc ~ fai_child_char + fai_caregiving + fai_percz_cure + 
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + malattia +
    quanti_fratelli,
  family = asym_laplace(),
  data = scaled_data, 
  backend = "cmdstanr"
  # algorithm = "meanfield"
)
```

```{r}
pp_check(mod_dep) 
```

```{r}
summary(mod_dep)
```

```{r}
bayes_R2(mod_dep)
```


```{r}
hist(scaled_data$dass_ax_sc)
```

```{r}
mod_ax <- brm(
  dass_ax_sc ~ fai_child_char + fai_caregiving + fai_percz_cure + 
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + malattia,
  family = asym_laplace(),
  data = scaled_data, 
  backend = "cmdstanr"
)
```

```{r}
pp_check(mod_ax) 
```

```{r}
summary(mod_ax)
```

```{r}
bayes_R2(mod_ax)
```


```{r}
hist(scaled_data$dass_str_sc)
```

```{r}
mod_str <- brm(
  dass_str_sc ~ fai_child_char + fai_caregiving + fai_percz_cure + 
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + malattia,
  family = asym_laplace(),
  data = scaled_data, 
  backend = "cmdstanr"
)
```

```{r}
pp_check(mod_str) 
```

```{r}
summary(mod_str)
```

```{r}
bayes_R2(mod_str)
```


```{r}
scaled_data$dass_tot <- scale(
  scaled_data$dass_de_sc + scaled_data$dass_ax_sc + scaled_data$dass_str_sc) |> 
  as.numeric()
```

```{r}
mod_dass_tot0 <- brm(
  dass_tot ~ fai_child_char + fai_caregiving + fai_percz_cure + 
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot,
  family = asym_laplace(),
  data = d_clean, 
  backend = "cmdstanr"
)
```


```{r}
mod_dass_tot <- brm(
  dass_tot ~ fai_child_char + fai_caregiving + fai_percz_cure + 
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + malattia +
    quanti_fratelli,
  family = asym_laplace(),
  data = d_clean, 
  backend = "cmdstanr"
)
```


```{r}
pp_check(mod_dass_tot) 
```

```{r}
summary(mod_dass_tot)
```

```{r}
bayes_R2(mod_dass_tot)
```



```{r}
mod_cope1 <- brm(
  cope_sc5 ~ fai_child_char + fai_caregiving + fai_percz_cure + 
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + malattia,
  family = asym_laplace(),
  data = fai, 
  backend = "cmdstanr"
)
```

```{r}
pp_check(mod_cope1) 
```

```{r}
summary(mod_cope1)
```

```{r}
bayes_R2(mod_dass_tot)
```



## Classification 


```{r}
# Set working directory for RedCap data
setwd("~/_repositories/meyer_fai/scripts/redcap")

# Source helper functions
source(here::here("scripts", "redcap", "_targets.R"))

# Read cleaned RedCap data
temp <- tar_read(fai_clean_complete)
# Remove problematic cases based on prior analysis
temp1 <- temp[-c(12, 18, 76, 81, 152, 182, 193, 202), ]
# Remove outlier identified previously
fai_clean_complete <- temp1[-92, ]
rm(temp, temp1)

# Select and rename relevant variables
fai <- fai_clean_complete |>
  dplyr::select(
    fai_child_charact, fai_caregiving, fai_cure, fai_intrapsych,
    fai_support, fai_hyper, quanti_fratelli, sesso, nome_malattia,
    eta_pz, legge_104
  ) |>
  dplyr::rename(
    fai_child_char = fai_child_charact,
    fai_caregiving = fai_caregiving,
    fai_percz_cure = fai_cure,
    fai_intra_psyc = fai_intrapsych,
    fai_soc_fam_sp = fai_support,
    fai_hyper_prot = fai_hyper
  )
```

Create target variable.

```{r}
fai <- fai |>
  mutate(
    nome_malattia_raggruppato = case_when(
      nome_malattia %in% c(
        "glioma", "LLA", "linfoma anaplastico", "Leucemia mieloide",
        "fibrosi cistica", "Pinealoblastoma",
        "tumore glioneurale leptomeningeale diffuso"
      ) ~ "Molto grave",

      nome_malattia %in% c(
        "Epilessia", "sindrome di charge", "Distrofia muscolare",
        "Neurofibromatosi", "diabete di tipo 1", "artrite romatoide",
        "RCU", "emofilia"
      ) ~ "Grave",

      nome_malattia %in% c(
        "asma", "Emicrania", "dermatite atopica", "ipospadia",
        "ipotonia muscolare", "Blefaro chetato congiuntivite"
      ) ~ "Moderata",

      TRUE ~ "Lieve"
    )
  )

fai$nome_malattia_raggruppato <- factor(fai$nome_malattia_raggruppato)
fai$nome_malattia_raggruppato <-
  relevel(fai$nome_malattia_raggruppato, ref = "Lieve")

fai$nome_malattia <- NULL
```

## Exploratory Data Analysis

### Check for Missing Values

```{r}
# Ensure no missing values are present
sum(is.na(fai))
```

### Inspect Class Distribution

```{r}
# Check class balance
table(fai$nome_malattia_raggruppato)

# Visualize class proportions
ggplot(data.frame(fai$nome_malattia_raggruppato),
       aes(x = fai$nome_malattia_raggruppato)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = "Class Distribution of Disease Severity",
    x = "Disease Severity",
    y = "Count"
  )
```

## Addressing Class Imbalance

Due to the extreme imbalance in class distribution, oversampling is used to balance the classes.

```{r}
set.seed(123)

# Balance dataset using oversampling
recipe_obj <- recipe(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving +
    fai_percz_cure + fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot +
    eta_pz + quanti_fratelli,
  data = fai
) %>%
  step_upsample(nome_malattia_raggruppato, over_ratio = 1)

balanced_fai <- prep(recipe_obj) %>% juice()

# Confirm balanced classes
table(balanced_fai$nome_malattia_raggruppato)
```

```{r}
# Visualize class distribution
balanced_fai |>
  ggplot(aes(x = nome_malattia_raggruppato)) +
  geom_bar(fill = "steelblue") +
  labs(
    title = "Class Distribution After Oversampling",
    x = "Illness Severity",
    y = "Count"
  )
```

## Predictive Modeling

### Multinomial Logistic Regression 

The Brant test showed violations of the proportional odds assumption, justifying the use of a multinomial logistic regression model.

```{r}
set.seed(123)

# Fit multinomial logistic regression
multinom_model <- nnet::multinom(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving +
    fai_percz_cure + fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot +
    eta_pz + quanti_fratelli,
  data = balanced_fai
)

# Model summary
summary(multinom_model)
```

#### Model Evaluation

Calculate z-values and p-values for model coefficients.

```{r}
# Evaluate model fit
z_values <- summary(multinom_model)$coefficients / summary(multinom_model)$standard.errors
p_values <- (1 - pnorm(abs(z_values), 0, 1)) * 2

# Display p-values
print(p_values)
```

Predictions and Confusion Matrix

```{r}
# Confusion matrix and accuracy
predicted <- predict(multinom_model, balanced_fai)
conf_matrix <- table(Predicted = predicted, Actual = balanced_fai$nome_malattia_raggruppato)
conf_matrix
accuracy <- mean(predicted == balanced_fai$nome_malattia_raggruppato)
cat("Multinomial Accuracy:", round(accuracy, 2))
```

Compare accuracy to baseline (random guessing).

```{r}
# Calculate the baseline accuracy (random guessing)
baseline_accuracy <- 1 / nlevels(balanced_fai$nome_malattia_raggruppato)
print(paste("Baseline Accuracy:", round(baseline_accuracy, 2)))
```

## Cross-Validation

```{r}
#| output: false
set.seed(123)

# Perform 10-fold cross-validation to evaluate the model's generalizability
train_control <- trainControl(method = "cv", number = 10)
cv_model <- train(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving +
    fai_percz_cure + fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot +
    eta_pz + quanti_fratelli,
  data = balanced_fai,
  method = "multinom",
  trControl = train_control
)
```

```{r}
# Print cross-validation results
print(cv_model)
```


## Relative Importance of Predictors

Display Variable Importance.

```{r}
set.seed(123)

# Assess variable importance using caret
importance_model <- varImp(cv_model, scale = FALSE)
print(importance_model)
```

Visualize relative importance.

```{r}
# Extract and reshape coefficient data for plotting
coef_data <- as.data.frame(t(summary(cv_model)$coefficients))
coef_data$Variable <- rownames(coef_data)

# Reshape the coefficients for visualization
coef_long <- reshape2::melt(coef_data, id.vars = "Variable")

# Plot absolute coefficient values for predictors
ggplot(
  coef_long,
  aes(x = reorder(Variable, abs(value)), y = abs(value), fill = variable)
) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(
    title = "Absolute Coefficients for Predictors",
    x = "Predictor",
    y = "Coefficient Magnitude",
    fill = "Outcome Category"
  )
```


### Nonparametric Machine Learning

Random Forest is used as a nonparametric alternative, as it can capture nonlinearities and interactions.

#### 1. Baseline Random Forest Model

- Purpose: Establish a benchmark for model performance on the full oversampled dataset.
- Justification: Offers insights into the maximum possible fit but risks overfitting as the model is trained and evaluated on the same data.
- Accuracy: High training accuracy due to full variability capture.
- Limitation: Overestimation of generalization performance as unseen data is not tested.

```{r}
# Fit Random Forest model
set.seed(123)

# Fit Random Forest model
rf_model <- randomForest(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + quanti_fratelli,
  data = balanced_fai,
  ntree = 500,
  importance = TRUE
)

# Variable importance
varImpPlot(rf_model)
```

Model Performance

```{r}
# Confusion matrix and accuracy
rf_predictions <- predict(rf_model, balanced_fai)
rf_conf_matrix <- table(Predicted = rf_predictions, Actual = balanced_fai$nome_malattia_raggruppato)
rf_conf_matrix
rf_accuracy <- sum(diag(rf_conf_matrix)) / sum(rf_conf_matrix)
cat("Random Forest Accuracy:", round(rf_accuracy, 2))
```

#### 2. Train-Test Split Random Forest

In this version, the data was split into a training set (70%) and a testing set (30%), and the RF model was refitted on the training data and evaluated on the test data. The key aspects include:

- Purpose: Introduce a more realistic evaluation by training on 70% of the data and testing on the remaining 30%.
- Justification: Prevents overfitting by validating on unseen data, providing an honest estimate of performance.
- Accuracy: Slightly reduced test accuracy compared to the baseline model, reflecting more realistic generalization.
- Limitation: Results depend on a single train-test split, which can introduce bias.

```{r}
set.seed(123)
train_index <- createDataPartition(
  balanced_fai$nome_malattia_raggruppato, p = 0.7, list = FALSE)
train_data <- balanced_fai[train_index, ]
test_data <- balanced_fai[-train_index, ]

# Refit the Random Forest model on the training data
rf_model <- randomForest(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + quanti_fratelli,
  data = train_data,
  ntree = 500,
  importance = TRUE
)

# Predict on the test data
rf_predictions_test <- predict(rf_model, test_data)

# Confusion matrix and accuracy
rf_conf_matrix_test <- table(Predicted = rf_predictions_test, Actual = test_data$nome_malattia_raggruppato)
rf_conf_matrix_test
rf_accuracy_test <- sum(diag(rf_conf_matrix_test)) / sum(rf_conf_matrix_test)
cat("Random Forest Test Accuracy:", round(rf_accuracy_test, 2))
```

```{r}
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)

cv_model <- train(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + quanti_fratelli,
  data = balanced_fai,
  method = "rf",
  trControl = train_control
)

print(cv_model)
```

#### 3. Regularized and Cross-Validated Random Forest

- Purpose: Optimize model generalization through regularization and evaluate robustness with 10-fold cross-validation.
- Justification: Regularization prevents overfitting, while cross-validation reduces bias in performance evaluation.
- Accuracy: Achieves robust accuracy (97%) across folds, demonstrating balanced complexity and predictive power.
- Limitation: Computationally intensive and may risk underfitting if hyperparameters are overly restrictive.

```{r}
set.seed(123)
rf_model_regularized <- randomForest(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + quanti_fratelli,
  data = train_data,
  ntree = 500,
  maxnodes = 20, # Restrict tree size
  mtry = 3 # Reduce predictors at each split
)

# Predict on test data
rf_predictions_test_regularized <- predict(rf_model_regularized, test_data)

# Confusion matrix and accuracy
rf_conf_matrix_test_regularized <- table(Predicted = rf_predictions_test_regularized, Actual = test_data$nome_malattia_raggruppato)
rf_accuracy_test_regularized <- sum(diag(rf_conf_matrix_test_regularized)) / sum(rf_conf_matrix_test_regularized)
cat("Random Forest Regularized Test Accuracy:", round(rf_accuracy_test_regularized, 2))
```

```{r}
# Load pdp package
library(pdp)

# Generate Partial Dependence Plot using pdp
pdp_caregiving <- partial(
  rf_model_regularized,
  pred.var = "fai_caregiving",
  train = balanced_fai
)
plotPartial(pdp_caregiving, main = "Partial Dependence: FAI Caregiving")
```

## Synthetic Minority Over-sampling Technique

```{r}
# Split the data into training and testing sets
set.seed(123)
train_index <- createDataPartition(fai$nome_malattia_raggruppato, p = 0.7, list = FALSE)
train_data <- fai[train_index, ]
test_data <- fai[-train_index, ]

# Define the control for training
train_control <- trainControl(method = "cv", number = 10, summaryFunction = defaultSummary)

# Baseline Random Forest model without handling class imbalance
set.seed(123)
baseline_model <- train(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + quanti_fratelli,
  data = train_data,
  method = "rf",
  trControl = train_control,
  metric = "Accuracy"
)
```

```{r}
# Separate features (X) and target variable (Y)
X <- train_data[, -which(names(train_data) == "nome_malattia_raggruppato")]
Y <- train_data$nome_malattia_raggruppato

# Ensure all features are numeric for SMOTE
# Convert categorical columns to dummy variables
dummies <- dummyVars(~ ., data = X)
X <- as.data.frame(predict(dummies, newdata = X))

# Apply SMOTE
set.seed(123)
smote_result <- SMOTE(X, Y, K = 5, dup_size = 1)

# Combine SMOTE results into a single dataset
smote_data <- data.frame(smote_result$data)
colnames(smote_data)[ncol(smote_data)] <- "nome_malattia_raggruppato"  # Rename the target column
smote_data$nome_malattia_raggruppato <- as.factor(smote_data$nome_malattia_raggruppato)  # Convert back to factor
```

```{r}
# Train the Random Forest model with SMOTE-processed data
smote_data <- na.omit(smote_data)

set.seed(123)
smote_model <- train(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + quanti_fratelli,
  data = smote_data,
  method = "rf",
  trControl = train_control,
  metric = "Accuracy"
)
```



```{r}
# Evaluate the baseline model
baseline_predictions <- predict(baseline_model, test_data)
baseline_conf_matrix <- confusionMatrix(baseline_predictions, test_data$nome_malattia_raggruppato)
print(baseline_conf_matrix)
```

```{r}
# Generate predictions on the test set
smote_predictions <- predict(smote_model, newdata = test_data)

```


```{r}
# Confusion matrix
confusion_matrix <- confusionMatrix(
  smote_predictions,
  test_data$nome_malattia_raggruppato
)

# Print confusion matrix and metrics
print(confusion_matrix)
```

```{r}
# Confusion matrix
confusion_matrix <- confusionMatrix(
  smote_predictions,
  test_data$nome_malattia_raggruppato
)

# Print confusion matrix and metrics
print(confusion_matrix)

```

```{r}
# Predictions for the baseline model
baseline_predictions <- predict(baseline_model, newdata = test_data)

# Evaluate the baseline model
baseline_confusion_matrix <- confusionMatrix(
  baseline_predictions,
  test_data$nome_malattia_raggruppato
)

# Compare Accuracy
cat("Baseline Model Accuracy:", baseline_confusion_matrix$overall["Accuracy"], "\n")
cat("SMOTE Model Accuracy:", confusion_matrix$overall["Accuracy"], "\n")

```

```{r}
# View cross-validation results
print(smote_model$results)

# Best-tuned parameters
print(smote_model$bestTune)
```




#### Final Recommendation

The Regularized CV RF Model is the most robust choice for practical applications. It achieves the highest accuracy on unseen data while mitigating overfitting through regularization and cross-validation. However, it is computationally more expensive, which should be considered when scaling to larger datasets.

## Discussion

1. **Multinomial Model**: The multinomial model provides insights into the relationship between FAI scales and illness severity. The violation of proportional odds justifies the use of this model.
2. **Random Forest**: The Random Forest model captures nonlinearities and provides variable importance.
3. **Combining Results**: The ensemble approach improves classification accuracy.

## Conclusion

Predictive Power of FAI Scales:

- Both models indicate that the FAI scales are strong predictors of illness severity.
- The Random Forest model, with near-perfect accuracy, demonstrates the predictive utility of the scales.

Applicability of the Models:

- The multinomial logistic regression model provides interpretable insights but is limited by its assumptions.
- The Random Forest model excels in predictive accuracy and robustness, making it suitable for practical applications.

In summary, the FAI scales demonstrate utility in predicting illness severity, but their discriminative ability varies across models. Future work should explore additional predictors or refine the classification.

## Final Conclusions

The analysis demonstrates the utility of the FAI scales in predicting illness severity. Among the predictors:

- fai_caregiving and fai_percz_cure are consistently the most significant, highlighting the centrality of caregiving-related factors.
- Demographic variables like the number of siblings and child age contribute minimally, reinforcing the scales' importance over general characteristics.

The regularized Random Forest model provides robust and interpretable results, achieving 97% test accuracy. These findings support the implementation of the FAI scales in practical contexts to identify caregivers requiring tailored interventions. Future studies could explore additional contextual variables and advanced modeling techniques to further enhance predictive accuracy.



```{r}
# Split the data into training and testing sets
set.seed(123)
train_index <- createDataPartition(fai$nome_malattia_raggruppato, p = 0.7, list = FALSE)
train_data <- fai[train_index, ]
test_data <- fai[-train_index, ]
```

```{r}
# Define the control for training
train_control <- trainControl(
  method = "cv",          # Cross-validation
  number = 10,            # 10-fold CV
  summaryFunction = defaultSummary # Default metrics (Accuracy and Kappa)
)
```


```{r}
# Train baseline Random Forest model without handling class imbalance
set.seed(123)
baseline_model <- train(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + quanti_fratelli,
  data = train_data,
  method = "rf",
  trControl = train_control,
  metric = "Accuracy"
)

# Handle any warnings about missing values in performance measures
if (!is.null(baseline_model$results)) {
  print("Baseline model trained successfully.")
} else {
  warning("Baseline model training encountered issues. Check training data and parameters.")
}

```

```{r}
# Separate features (X) and target variable (Y)
X <- train_data[, -which(names(train_data) == "nome_malattia_raggruppato")]
Y <- train_data$nome_malattia_raggruppato

# Convert categorical columns to dummy variables for SMOTE
dummies <- dummyVars(~ ., data = X)
X <- as.data.frame(predict(dummies, newdata = X))
```

```{r}
# Apply SMOTE with appropriate K
set.seed(123)
tryCatch({
  smote_result <- SMOTE(X, Y, K = 3, dup_size = 1)  # Adjust K if dataset is small
}, error = function(e) {
  stop("SMOTE failed: ", e$message)
})

# Combine SMOTE results into a single dataset
smote_data <- data.frame(smote_result$data)
colnames(smote_data)[ncol(smote_data)] <- "nome_malattia_raggruppato"  # Rename target
smote_data$nome_malattia_raggruppato <- as.factor(smote_data$nome_malattia_raggruppato)  # Convert to factor
smote_data <- na.omit(smote_data)  # Remove rows with missing values
```

```{r}
# Train the Random Forest model with SMOTE-processed data
set.seed(123)
smote_model <- train(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + quanti_fratelli,
  data = smote_data,
  method = "rf",
  trControl = train_control,
  metric = "Accuracy"
)
```

```{r}
# Evaluate baseline model
baseline_predictions <- predict(baseline_model, newdata = test_data)
baseline_conf_matrix <- confusionMatrix(baseline_predictions, test_data$nome_malattia_raggruppato)
print("Baseline Model Performance:")
print(baseline_conf_matrix)

# Evaluate SMOTE-enhanced model
smote_predictions <- predict(smote_model, newdata = test_data)
smote_conf_matrix <- confusionMatrix(smote_predictions, test_data$nome_malattia_raggruppato)
print("SMOTE Model Performance:")
print(smote_conf_matrix)

# Compare Accuracy
cat("Baseline Model Accuracy:", baseline_conf_matrix$overall["Accuracy"], "\n")
cat("SMOTE Model Accuracy:", smote_conf_matrix$overall["Accuracy"], "\n")
```

```{r}
# View cross-validation results
print("SMOTE Model Cross-Validation Results:")
print(smote_model$results)

# Best-tuned parameters
print("Best Parameters for SMOTE Model:")
print(smote_model$bestTune)
```


---------------


```{r}
# Combine classes into two groups: Lieve (low risk) vs. others (high risk)
fai$nome_malattia_raggruppato <- factor(
  ifelse(fai$nome_malattia_raggruppato == "Lieve", "Low Risk", "High Risk")
)

# Check the new class distribution
table(fai$nome_malattia_raggruppato)
```

```{r}
library(ROSE)

# Split data into training and testing sets
set.seed(123)
train_index <- createDataPartition(fai$nome_malattia_raggruppato, p = 0.7, list = FALSE)
train_data <- fai[train_index, ]
test_data <- fai[-train_index, ]

# Apply ROSE to balance the training data
set.seed(123)
train_data_rose <- ROSE(nome_malattia_raggruppato ~ ., data = train_data, seed = 123)$data

# Check the new class distribution
table(train_data_rose$nome_malattia_raggruppato)
```

```{r}
library(xgboost)
library(caret)

# Convert the training data to a matrix format
X_train <- model.matrix(nome_malattia_raggruppato ~ . - 1, data = train_data_rose)
y_train <- train_data_rose$nome_malattia_raggruppato

X_test <- model.matrix(nome_malattia_raggruppato ~ . - 1, data = test_data)
y_test <- test_data$nome_malattia_raggruppato

# Train an XGBoost model
set.seed(123)
xgb_model <- xgboost(
  data = as.matrix(X_train),
  label = as.numeric(y_train) - 1, # Convert factor levels to 0, 1, 2...
  objective = "multi:softprob",    # Multiclass classification
  num_class = length(levels(y_train)),
  nrounds = 100,
  eta = 0.1,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8,
  eval_metric = "mlogloss"
)

# Save model for future use
saveRDS(xgb_model, "xgb_model.rds")
```

```{r}
library(pROC)

# Make predictions
xgb_predictions <- predict(xgb_model, as.matrix(X_test))
xgb_predictions <- matrix(xgb_predictions, ncol = length(levels(y_test)), byrow = TRUE)
predicted_classes <- apply(xgb_predictions, 1, which.max)
predicted_classes <- factor(predicted_classes, labels = levels(y_test))

# Confusion Matrix
confusion_matrix <- confusionMatrix(predicted_classes, y_test)
print(confusion_matrix)

# Calculate precision, recall, and F1-score
precision <- posPredValue(predicted_classes, y_test, positive = "High Risk")
recall <- sensitivity(predicted_classes, y_test, positive = "High Risk")
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1_score, "\n")
```

```{r}
var_importance <- xgb.importance(model = xgb_model)
print(var_importance)
xgb.plot.importance(var_importance)
```

```{r}
train_data_rose <- ROSE(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + 
    quanti_fratelli, 
    data = train_data, seed = 123, N = nrow(train_data))$data
```

```{r}
rf_model <- randomForest(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + eta_pz + 
    quanti_fratelli,
  data = train_data,
  classwt = c("Low Risk" = 0.5, "High Risk" = 2)  # Adjust weights
)
```

```{r}
xgb_model <- xgboost(
  data = as.matrix(X_train),
  label = as.numeric(y_train) - 1,
  objective = "binary:logistic",
  scale_pos_weight = sum(y_train == "Low Risk") / sum(y_train == "High Risk"),
  nrounds = 100
)
```

```{r}
# Predict on the test set using Random Forest
rf_predictions <- predict(rf_model, newdata = test_data)

# Confusion Matrix and Metrics for Random Forest
rf_conf_matrix <- confusionMatrix(rf_predictions, test_data$nome_malattia_raggruppato)
print("Random Forest Model Performance:")
print(rf_conf_matrix)

# Extract precision, recall, and F1-score for Random Forest
rf_precision <- posPredValue(rf_predictions, test_data$nome_malattia_raggruppato, positive = "High Risk")
rf_recall <- sensitivity(rf_predictions, test_data$nome_malattia_raggruppato, positive = "High Risk")
rf_f1_score <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)

cat("Random Forest - Precision:", rf_precision, "\n")
cat("Random Forest - Recall:", rf_recall, "\n")
cat("Random Forest - F1 Score:", rf_f1_score, "\n")
```

```{r}
# Predict probabilities
rf_probabilities <- predict(rf_model, newdata = test_data, type = "prob")[, "High Risk"]

# Adjust threshold
adjusted_threshold <- 0.3  # Example: Lower the threshold to favor minority class
rf_predictions_adjusted <- ifelse(rf_probabilities > adjusted_threshold, "High Risk", "Low Risk")
rf_predictions_adjusted <- factor(rf_predictions_adjusted, levels = levels(test_data$nome_malattia_raggruppato))

# Evaluate with adjusted threshold
rf_conf_matrix_adjusted <- confusionMatrix(rf_predictions_adjusted, test_data$nome_malattia_raggruppato)
print("Random Forest Model with Adjusted Threshold:")
print(rf_conf_matrix_adjusted)
```


```{r}
train_data_rose <- ROSE(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + 
    quanti_fratelli, 
  data = train_data, seed = 123, N = 2 * nrow(train_data)  # Increase synthetic data size
)$data
```

```{r}
rf_model <- randomForest(
  nome_malattia_raggruppato ~ fai_child_char + fai_caregiving + fai_percz_cure +
    fai_intra_psyc + fai_soc_fam_sp + fai_hyper_prot + 
    quanti_fratelli,
  data = train_data_rose,
  classwt = c("Low Risk" = 0.3, "High Risk" = 2.5)  # Increase penalty for "High Risk"
)
```

```{r}
# Random Forest Feature Importance
importance_rf <- varImp(rf_model)
print(importance_rf)
plot(importance_rf)
```

```{r}
# Predict on the test set
rf_predictions <- predict(rf_model, newdata = test_data)

# Confusion Matrix
rf_conf_matrix <- confusionMatrix(rf_predictions, test_data$nome_malattia_raggruppato)

# Print Confusion Matrix and Statistics
print("Random Forest Model Performance:")
print(rf_conf_matrix)

# Extract metrics from the confusion matrix
rf_precision <- posPredValue(rf_predictions, test_data$nome_malattia_raggruppato, positive = "High Risk")
rf_recall <- sensitivity(rf_predictions, test_data$nome_malattia_raggruppato, positive = "High Risk")
rf_f1_score <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)

# Print Precision, Recall, and F1 Score
cat("Precision:", rf_precision, "\n")
cat("Recall:", rf_recall, "\n")
cat("F1 Score:", rf_f1_score, "\n")
```

```{r}
# Predict probabilities for the test set
rf_probabilities <- predict(rf_model, newdata = test_data, type = "prob")[, "High Risk"]

# Adjust threshold
adjusted_threshold <- 0.3  # Example threshold
rf_predictions_adjusted <- ifelse(rf_probabilities > adjusted_threshold, "High Risk", "Low Risk")
rf_predictions_adjusted <- factor(rf_predictions_adjusted, levels = levels(test_data$nome_malattia_raggruppato))

# Confusion Matrix with adjusted threshold
rf_conf_matrix_adjusted <- confusionMatrix(rf_predictions_adjusted, test_data$nome_malattia_raggruppato)

# Print Confusion Matrix and Metrics
print("Random Forest Model Performance with Adjusted Threshold:")
print(rf_conf_matrix_adjusted)
```

```{r}
# ROC Curve
rf_roc <- roc(test_data$nome_malattia_raggruppato, rf_probabilities, levels = c("Low Risk", "High Risk"))
plot(rf_roc, col = "blue", main = "ROC Curve - Random Forest")
auc_rf <- auc(rf_roc)

# Print AUC
cat("AUC:", auc_rf, "\n")
```

